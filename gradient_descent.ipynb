{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcQKB0YdOxHwXGA07P9Ug8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Klu77/project-1-for-Git-tools/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZMiUuRaiJtL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GrandmaCan/ML/main/Resgression/Salary_Data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data\n",
        "# y = w*x + b\n",
        "x = data['YearsExperience']\n",
        "y = data['Salary']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, w, b):\n",
        "  y_pred = w*x + b\n",
        "  cost = (y - y_pred)**2\n",
        "  cost = cost.sum() / len(x)\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "ATNyVQBatxt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x, y, w, b):\n",
        "  w_gradient = (2*x*(w*x+b -y)).mean()\n",
        "  b_gradient = (2*(w*x+b -y)).mean()\n",
        "  return w_gradient, b_gradient"
      ],
      "metadata": {
        "id": "5bcvfYib5G4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "w_gradient = 2*x*(w*x+b -y)\n",
        "b_gradient = 2*(w*x+b -y)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nnCmeMFKkyHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cadf25fe-3862-4a6d-fbfe-29afab10a1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nw_gradient = 2*x*(w*x+b -y)\\nb_gradient = 2*(w*x+b -y)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 假设你有 w 和 b 的初始值\n",
        "w = 0\n",
        "b = 0\n",
        "#num_iterations = 20\n",
        "learning_rate = 0.01\n",
        "\n",
        "def gradient_descent(\n",
        "    x, y, w_init, b_init, learning_rate,\n",
        "    cost_function, gradient_function,\n",
        "    run_inter, p_inter\n",
        "):\n",
        "  w = w_init\n",
        "  b = b_init\n",
        "  #建立列表c_hist, w_hist and b_hist，分别存储每次迭代计算中的cost, w and b\n",
        "  c_hist = []\n",
        "  w_hist = []\n",
        "  b_hist = []\n",
        "\n",
        "  for i in range(run_inter):\n",
        "    # 对于数据集中的每个点计算梯度\n",
        "    #w_gradient = 2 * x * (w * x + b - y)\n",
        "    #b_gradient = 2 * (w * x + b - y)\n",
        "\n",
        "    #可以把2删掉 learning_rate  = 0.002\n",
        "    w_gradient = x * (w * x + b - y)\n",
        "    b_gradient = w * x + b - y\n",
        "\n",
        "    # 计算平均梯度\n",
        "    w_gradient = w_gradient.mean()\n",
        "    b_gradient = b_gradient.mean()\n",
        "    '''\n",
        "    梯度下降是一种寻找函数最小值（或最大值）的优化算法，常用于机器学习和深度学习中的模型训练。当你使用梯度下降来训练模型时，\n",
        "    你的目标是找到一组参数（在线性回归中是 w 和b），使得损失函数的值最小。计算平均梯度是一种通用的做法，\n",
        "    不仅适用于简单的线性回归模型，也适用于更复杂的模型和损失函数。这种方法为各种类型的模型提供了一种稳健的参数更新策略\n",
        "    注释也要注意缩进\n",
        "    '''\n",
        "    # 更新 w 和 b\n",
        "    w = w - learning_rate * w_gradient\n",
        "    b = b - learning_rate * b_gradient\n",
        "\n",
        "    cost = compute_cost(x, y, w, b)\n",
        "\n",
        "    w_hist.append(w)\n",
        "    b_hist.append(b)\n",
        "    c_hist.append(cost)\n",
        "\n",
        "    if i%p_inter ==0: # 每隔inter打印一次，即不是每次都打印\n",
        "      print(\n",
        "           f\"interation{i:8} : Cost {cost:.2f}, w: {w:.2f}, b: {b:.2f}\"\n",
        "          ) #i;5 是为了为了对其输出 :.2f是为了小数点位数\n",
        "  return w, b, w_hist, b_hist, c_hist\n"
      ],
      "metadata": {
        "id": "1N3lcdfXsr6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这句话使用了 Python 3.6+ 中引入的 f-string（格式化字符串字面值）功能来构造字符串。f-string 提供了一种快速而简洁的方式来嵌入表达式到字符串字面值中。语法上，f-string 使用一对大括号 `{}` 来标记那些需要被 Python 表达式替换的地方，而字符串需要以 `f` 或 `F` 为前缀。\n",
        "\n",
        "以下是这句话的组成部分：\n",
        "\n",
        "- `f\"`：这是 f-string 的开始，意味着该字符串将被格式化。\n",
        "- `iteration{i}`：`{i}` 会被替换为变量 `i` 的值。如果 `i` 的值是 1，则这部分会显示为 \"iteration1\"。\n",
        "- `: Cost{cost}`：类似地，`{cost}` 会被替换为变量 `cost` 的值。如果 `cost` 的值是 300，则这部分会显示为 \": Cost300\"。\n",
        "- `, w:{w}`：`{w}` 会被替换为变量 `w` 的值。\n",
        "- `, b:{b}`：`{b}` 会被替换为变量 `b` 的值。\n",
        "- `\"`：字符串的结尾。\n",
        "\n",
        "当这个 f-string 被 Python 解释器处理时，大括号里的表达式会被求值，然后它们的结果会被转换为字符串并插入到最终的字符串中。例如，如果 `i` 是 1，`cost` 是 300.0，`w` 是 0.5，而 `b` 是 1，那么这个 f-string 将会生成并打印出如下字符串：\n",
        "\n",
        "```\n",
        "iteration1 : Cost300.0, w:0.5, b:1\n",
        "```\n",
        "\n",
        "f-string 是一种非常有用的特性，因为它可以在不牺牲可读性的情况下使字符串格式化变得非常简洁和直观。"
      ],
      "metadata": {
        "id": "JAEylmDcv9FD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在批量梯度下降中，计算平均梯度是为了基于整个数据集对模型参数进行一次更新，这种方法考虑了所有数据点对损失函数的贡献。平均梯度是对每个数据点计算的梯度的平均值，它指示了在参数空间中减少损失函数的整体方向。\n",
        "\n",
        "计算平均梯度的理由有几个：\n",
        "\n",
        "1. **稳定性**：使用整个数据集的平均梯度来更新参数可以提供一个稳定的梯度估计，因为它减少了单个数据点可能引起的高方差。\n",
        "\n",
        "2. **准确性**：当你考虑整个数据集时，平均梯度能更准确地代表总体的损失梯度，而不是仅仅基于单个样本或小批样本的梯度。\n",
        "\n",
        "3. **全局最优**：批量梯度下降旨在寻找全局最优解，而非只是局部最优。平均梯度有助于指导模型参数朝向损失函数的全局最小值移动。\n",
        "\n",
        "4. **效率**：尽管计算整个数据集的梯度比单个数据点的计算成本要高，但这种方法在每次迭代中只需要一次参数更新，从而可以在较少的迭代次数内收敛。\n",
        "\n",
        "然而，值得注意的是，批量梯度下降也有其缺点。当数据集非常大时，计算整个数据集的梯度可能会非常耗时，并且对内存的要求也会很高。在这种情况下，通常会使用随机梯度下降（SGD）或小批量梯度下降（Mini-batch GD），这两种方法只考虑一个或一小部分样本来计算梯度，可以更快地进行迭代，虽然可能会增加收敛到最小值的迭代次数。"
      ],
      "metadata": {
        "id": "_Zry_PkUsmE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_init = 0\n",
        "b_init = 0\n",
        "learning_rate = 1.0e-3\n",
        "run_inter = 20000\n",
        "p_inter = 1000\n",
        "\n",
        "w_final, b_final, w_hist, b_hist, c_hist = gradient_descent(\n",
        "    x, y, w_init, b_init, learning_rate,\n",
        "    compute_cost, compute_gradient,\n",
        "    run_inter, p_inter\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5DRp2DUwbvg",
        "outputId": "79638f1b-05ed-4fb9-e9ca-c6934dc7872a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "interation       0 : Cost 5656.80, w: 0.44, b: 0.07\n",
            "interation    1000 : Cost 140.97, w: 12.12, b: 8.07\n",
            "interation    2000 : Cost 96.06, w: 11.42, b: 12.74\n",
            "interation    3000 : Cost 69.73, w: 10.88, b: 16.32\n",
            "interation    4000 : Cost 54.29, w: 10.47, b: 19.06\n",
            "interation    5000 : Cost 45.24, w: 10.15, b: 21.15\n",
            "interation    6000 : Cost 39.93, w: 9.91, b: 22.76\n",
            "interation    7000 : Cost 36.82, w: 9.73, b: 23.99\n",
            "interation    8000 : Cost 35.00, w: 9.59, b: 24.93\n",
            "interation    9000 : Cost 33.93, w: 9.48, b: 25.65\n",
            "interation   10000 : Cost 33.30, w: 9.39, b: 26.21\n",
            "interation   11000 : Cost 32.93, w: 9.33, b: 26.63\n",
            "interation   12000 : Cost 32.72, w: 9.28, b: 26.95\n",
            "interation   13000 : Cost 32.59, w: 9.25, b: 27.20\n",
            "interation   14000 : Cost 32.51, w: 9.22, b: 27.39\n",
            "interation   15000 : Cost 32.47, w: 9.20, b: 27.54\n",
            "interation   16000 : Cost 32.45, w: 9.18, b: 27.65\n",
            "interation   17000 : Cost 32.43, w: 9.17, b: 27.73\n",
            "interation   18000 : Cost 32.42, w: 9.16, b: 27.80\n",
            "interation   19000 : Cost 32.42, w: 9.15, b: 27.85\n"
          ]
        }
      ]
    }
  ]
}